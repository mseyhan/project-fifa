{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# DO NOT FORGET TO CHECK IF THERE ARE MISSING PLAYER INFORMATION, SUCH AS MISSING POSITIONS. XPATH COULD BE DIFFERENT FOR A NUMBER OF PLAYERS.\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "# required imports\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "import undetected_chromedriver as uc\n",
    "import re\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "from dateutil import relativedelta\n",
    "import numpy as np\n",
    "import sqlite3 as sql3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# var definitions\n",
    "\n",
    "PATH = f\"{os.path.abspath('')}\\output\"\n",
    "FIX_PATH = f\"{os.path.abspath('')}\\output\"\n",
    "\n",
    "stat_dict = {}\n",
    "counter = 0\n",
    "master_dict = {}\n",
    "refined_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate WHOLE RAW DATABASE INTO ONE (there will be several files like \"raw_data2023-08-19-2037.pkl\")\n",
    "# if you get a PermissionError, check file locations on your computer or var definitions\n",
    "\n",
    "master_raw_data = []\n",
    "for file in os.listdir(PATH):\n",
    "    full_path = os.path.join(PATH,file) # find the raw outputs resulted by scraping and join\n",
    "    with open(full_path,\"rb\") as fhand:\n",
    "        raw_data = pickle.load(fhand)\n",
    "    master_raw_data.extend(raw_data)\n",
    "print(len(master_raw_data))\n",
    "with open(\"master_raw_data.pkl\",\"wb\") as fhand:\n",
    "    pickle.dump(master_raw_data,fhand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data engineering and forming\n",
    "\n",
    "master_data = []\n",
    "\n",
    "with open(\"output/master_raw_data.pkl\",\"rb\") as fhand:\n",
    "    full_raw_data = pickle.load(fhand) # load full raw data\n",
    "\n",
    "for index in range(0,len(full_raw_data),2):\n",
    "\n",
    "    for i in range(0,len(full_raw_data[index+1]),2):\n",
    "        stat_dict[full_raw_data[index+1][i]] = full_raw_data[index+1][i+1]\n",
    "        full_raw_data[index] = full_raw_data[index] | stat_dict\n",
    "        stat_dict = {} \n",
    "\n",
    "    master_data.append(full_raw_data[index]) # turn lists into dictionaries (old form was {},[] per player) and merge the two dictionaries, making one full dictionary for one player\n",
    "\n",
    "\n",
    "#### Execute here if you do not have a \"master_data.pkl\" file!\n",
    "# df = pd.DataFrame(master_data)\n",
    "# with open(\"output/master_data.pkl\",\"wb\") as fhand:\n",
    "#   pickle.dump(df,fhand)        \n",
    "\n",
    "for i in master_data:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge main df and the link df\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "with open(\"output/master_data.pkl\",\"rb\") as fhand:\n",
    "    main_df = pickle.load(fhand) # Load the main dataframe\n",
    "    \n",
    "with open(\"data/player_links.pkl\",\"rb\") as fhand:\n",
    "    link_dict = pickle.load(fhand) # Load the link dictionary\n",
    "\n",
    "print(link_dict)\n",
    "\n",
    "print(len([x for x in link_dict]))\n",
    "\n",
    "link_df = pd.DataFrame([x for x in link_dict.values()],index=[x for x in link_dict]) # Create a link dataframe from the link dictionary\n",
    "\n",
    "frames = [main_df,link_df]\n",
    "\n",
    "link_df = link_df.reset_index(names=\"playerlink\") # since player links already exist in main_df, reset the link index on newly created link dataframe for merging\n",
    "\n",
    "mdf = pd.merge(main_df,link_df,on=\"playerlink\", how=\"left\") # Merge the two dictionaries\n",
    "mdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column removal (\"price updated\", columns with same headers)\n",
    "\n",
    "mdf.drop(columns=[\"Price Updated\"],inplace=True)\n",
    "mdf.drop(columns=[\"DIVING\",\"HANDLING\",\"KICKING\",\"REFLEXES\",\"POSITIONING\"],inplace=True)\n",
    "\n",
    "mdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the overall rating column name and upper column names\n",
    "\n",
    "new_column_name = 'Overall Rating'\n",
    "mdf.rename(columns={0: new_column_name}, inplace=True)\n",
    "\n",
    "for x in mdf.columns:\n",
    "    if x == x.upper() and not x == \"ID\" and not x == \"DOB\":\n",
    "        renamedict = {}\n",
    "        renamedict[x] = x.title()\n",
    "        mdf.rename(columns=renamedict,inplace=True) # Detect full capitalized columns like \"PACE\", exclude \"ID\" and \"DOB\" and rename remaining\n",
    "mdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add height by cm and in\n",
    "\n",
    "cm_list = []\n",
    "in_list = []\n",
    "str_ex = r'''([0-z]+'[0-z]+\")'''\n",
    "\n",
    "\n",
    "if mdf.columns[10] == \"Height\":\n",
    "\n",
    "    for x in mdf.loc[:,\"Height\"]:\n",
    "        stg = re.findall(\".+?[0-9]+\", x)[0]\n",
    "        cm_list.append(stg) # Select cm values in the strings, eg. select aaa in \"aaacm | bbbin\"\n",
    "\n",
    "    for x in mdf.loc[:, \"Height\"]:\n",
    "        stg = re.findall(str_ex, x)[0]\n",
    "        stg_without_backslash = stg.replace(r'\\\\', '')  # Remove backslashes from the captured value\n",
    "        in_list.append(stg_without_backslash)\n",
    "\n",
    "    n = mdf.columns[10]\n",
    "    mdf.drop(n, axis = 1, inplace=True) # Drop the old height values\n",
    "    mdf[n] = cm_list\n",
    "    mdf.rename(columns={\"Height\": \"Height (cm)\"}, inplace=True) # Create the cm column\n",
    "    mdf[\"Height (in)\"] = in_list # Create the in column\n",
    "else:\n",
    "    pass\n",
    "\n",
    "mdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix Date of Birth/Age\n",
    "converted_ages = []\n",
    "counter = 0\n",
    "\n",
    "def date_to_age(string):\n",
    "    stg = re.findall(\"[0-9]+\",string)\n",
    "    final_str = f\"{stg[2]}-{stg[1]}-{stg[0]}\" # Date conversion from DD-MM-YYYY to YYYY-MM-DD\n",
    "\n",
    "    today_date = datetime.strptime(str(date.today()), \"%Y-%m-%d\") \n",
    "    date_of_birth = datetime.strptime(final_str, \"%Y-%m-%d\") # convert strings to datetime objects\n",
    "\n",
    "    delta = relativedelta.relativedelta(today_date,date_of_birth) # find the difference between two dates\n",
    "\n",
    "    converted_ages.append(f\"{delta.years} years old\")\n",
    "\n",
    "for index, value in enumerate(mdf.loc[:, \"DOB\"]):\n",
    "    if not pd.isnull(value):\n",
    "        date_to_age(value)\n",
    "        age = converted_ages[counter]\n",
    "        counter += 1\n",
    "        mdf.loc[index, \"Age\"] = age # merge modified date of birth values to age\n",
    "\n",
    "for index, value in enumerate(mdf.loc[:,\"Age\"]):\n",
    "    str_ex = re.findall(\"[0-9]+\", value)\n",
    "    mdf.loc[index, \"Age\"] = str_ex[0] # remove \"years old\" strings\n",
    "\n",
    "for x in mdf.loc[:,\"Age\"]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if weight = 0 -> null\n",
    "\n",
    "for index, value in enumerate(mdf.loc[:,\"Weight\"]):\n",
    "    if value == \"0\":\n",
    "        mdf.loc[index, \"Weight\"] = np.nan # find \"0\"s in weight values and convert them to np.nan\n",
    "\n",
    "for x in mdf.loc[:, \"Weight\"]:\n",
    "    if type(x) == float:\n",
    "        print(\"nan found\") # checker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if origin = \"N\\A\" -> null\n",
    "\n",
    "for index, value in enumerate(mdf.loc[:,\"Origin\"]):\n",
    "    if value == \"N\\A\":\n",
    "        mdf.loc[index, \"Origin\"] = np.nan # find \"N\\A\"s in origin values and convert them to np.nan\n",
    "\n",
    "for x in mdf.loc[:, \"Origin\"]:\n",
    "    if type(x) == float:\n",
    "        print(\"nan found\") # checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change data type of numbers and the same named columns\n",
    "\n",
    "mdf.columns.values[39] = \"Dribbling Main\"\n",
    "mdf.columns.values[44] = \"Dribbling Alt\"\n",
    "mdf.columns    \n",
    "\n",
    "counter = 0\n",
    "\n",
    "for x in mdf.columns:\n",
    "    for index, value in enumerate(mdf.loc[:, x]):\n",
    "        counter += 1\n",
    "        if isinstance(value, str) and value.isdigit(): #check if the \"str\" can be an \"int\"\n",
    "            mdf.loc[index, x] = int(value)\n",
    "        print(type(mdf.loc[index, x]))\n",
    "    print(f\"checked {x}\")\n",
    "\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column \"Date Added\" (Date for testing: 2023-08-25)\n",
    "# THIS BLOCK IS FOR TESTING ONLY. ACTUAL \"Date Added\" will be implemented in a different way.\n",
    "\n",
    "test_date_obj = datetime.strptime(\"2023-08-25\",\"%Y-%m-%d\").date()\n",
    "date_list = []\n",
    "\n",
    "for i in mdf.loc[:,\"Name\"]:\n",
    "    date_list.append(test_date_obj)\n",
    "mdf[\"Date Added\"] = date_list\n",
    "mdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the Master Dataframe\n",
    "\n",
    "with open(\"output/master_dataframe.pkl\",\"wb\") as fhand:\n",
    "    pickle.dump(mdf,fhand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpickle existing & modified Master Dataframe\n",
    "\n",
    "with open(\"output/master_dataframe.pkl\",\"rb\") as fhand:\n",
    "    mdf = pickle.load(fhand)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQLite Writer\n",
    "\n",
    "conn = sql3.connect('database.db')\n",
    "\n",
    "mdf.to_sql(name=\"player_database\",con=conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excel Writer\n",
    "\n",
    "writer = pd.ExcelWriter('data/player_master_df.xlsx')# pylint: disable=abstract-class-instantiated\n",
    "mdf.to_excel(writer, sheet_name=\"sheet1\",index=False)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the indices of \"Dribbling\" columns\n",
    "\n",
    "# Get boolean array indicating columns with the name \"Dribbling\"\n",
    "dribbling_mask = mdf.columns == \"Dribbling\"\n",
    "\n",
    "# Get index positions where \"Dribbling\" columns are located\n",
    "dribbling_indices = [i for i, value in enumerate(dribbling_mask) if value]\n",
    "\n",
    "print(dribbling_indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
